{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "59e1daa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/qik/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import nltk\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import re\n",
    "from typing import List, Tuple, Dict\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 20\n",
    "learning_rate = 2e-4\n",
    "target_confidence = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b9296b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = ' '.join(text.split())\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s.,!?\\'\":]', '', text)\n",
    "    return text\n",
    "\n",
    "def train_word2vec(data_dir, embedding_dim=300):\n",
    "    sentences = []\n",
    "    for filename in [\"sentiment.train.0\", \"sentiment.train.1\"]:\n",
    "        file_path = os.path.join(data_dir, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                text = preprocess_text(line)\n",
    "                sentences.append(text.strip().split())\n",
    "    \n",
    "    model = Word2Vec(sentences=sentences, vector_size=embedding_dim, window=5, min_count=1, workers=4)\n",
    "    return model\n",
    "\n",
    "def build_vocab(data_dir: str, vocab_size: int = 10000, min_freq: int = 2) -> Dict[str, int]:\n",
    "    word_counter = Counter()\n",
    "    special_tokens = {\n",
    "        '<PAD>': 0,\n",
    "        '<UNK>': 1,\n",
    "        '<BOS>': 2,\n",
    "        '<EOS>': 3,\n",
    "    }\n",
    "    \n",
    "    for filename in [\"sentiment.train.0\", \"sentiment.train.1\"]:\n",
    "        file_path = os.path.join(data_dir, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                text = preprocess_text(line)\n",
    "                words = text.strip().split()\n",
    "                word_counter.update(words)\n",
    "    \n",
    "    filtered_words = [(word, count) for word, count in word_counter.items() \n",
    "                     if count >= min_freq]\n",
    "    sorted_words = sorted(filtered_words, key=lambda x: x[1], reverse=True)\n",
    "    most_common_words = sorted_words[:vocab_size - len(special_tokens)]\n",
    "    \n",
    "    vocab = dict(special_tokens)\n",
    "    for idx, (word, _) in enumerate(most_common_words, start=len(special_tokens)):\n",
    "        vocab[word] = idx\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "def initialize_embeddings(vocab, word2vec_model):\n",
    "    embedding_matrix = torch.zeros((len(vocab), word2vec_model.vector_size))\n",
    "    \n",
    "    for word, idx in vocab.items():\n",
    "        try:\n",
    "            if word in word2vec_model.wv:\n",
    "                embedding_matrix[idx] = torch.FloatTensor(word2vec_model.wv[word])\n",
    "            else:\n",
    "                embedding_matrix[idx] = torch.randn(word2vec_model.vector_size) * 0.1\n",
    "        except KeyError:\n",
    "            embedding_matrix[idx] = torch.randn(word2vec_model.vector_size) * 0.1\n",
    "    \n",
    "    return embedding_matrix\n",
    "\n",
    "class TextDatasetTest(Dataset):\n",
    "    def __init__(self, data_dir: str, vocab: Dict[str, int], max_length: int = 100, is_train: bool = True):\n",
    "        super().__init__()\n",
    "        self.data = []\n",
    "        self.vocab = vocab\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        if is_train:\n",
    "            files = [\"sentiment.train.0\", \"sentiment.train.1\"]\n",
    "        else:\n",
    "            files = [\"sentiment.test.0\", \"sentiment.test.1\"]\n",
    "        for filename in files:\n",
    "            file_path = os.path.join(data_dir, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                lines = f.readlines()\n",
    "                for line in lines:\n",
    "                    text = preprocess_text(line)\n",
    "                    tokens = text.strip().split()\n",
    "                    if len(tokens) > max_length:\n",
    "                        tokens = tokens[:max_length]\n",
    "                    label = 1 if filename.endswith('.1') else 0\n",
    "                    self.data.append((tokens, label))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tokens, label = self.data[idx]\n",
    "        token_ids = [self.vocab.get(token, self.vocab['<UNK>']) for token in tokens]\n",
    "        return torch.tensor(token_ids, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inputs, labels = zip(*batch)\n",
    "    max_length = max(len(seq) for seq in inputs)\n",
    "    \n",
    "    padded_inputs = [torch.cat([seq, torch.zeros(max_length - len(seq), dtype=torch.long)]) for seq in inputs]\n",
    "    lengths = [len(seq) for seq in inputs]\n",
    "    \n",
    "    return torch.stack(padded_inputs), torch.tensor(labels, dtype=torch.float), lengths\n",
    "\n",
    "# Initialize data\n",
    "data_dir = \"./data/sentiment_style_transfer/yelp\"\n",
    "word2vec_model = train_word2vec(data_dir)\n",
    "vocab = build_vocab(data_dir)\n",
    "embedding_matrix = initialize_embeddings(vocab, word2vec_model)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "dataset = TextDatasetTest(data_dir, vocab, is_train=True)\n",
    "data_loader = DataLoader(dataset, batch_size=64, collate_fn=collate_fn, shuffle=True, num_workers=0)\n",
    "val_dataset = TextDatasetTest(data_dir, vocab, is_train=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, collate_fn=collate_fn, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d87d2a9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "443259"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dceba55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class StyleClassifier(nn.Module):\n",
    "    def __init__(self, style_dim, num_classes):\n",
    "        \"\"\"\n",
    "        Style Classifier to predict style labels from the style embedding.\n",
    "\n",
    "        Args:\n",
    "            style_dim: Dimensionality of the style embedding (input size).\n",
    "            num_classes: Number of style classes (output size).\n",
    "        \"\"\"\n",
    "        super(StyleClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(style_dim, 64)  # Hidden layer with 64 units\n",
    "        self.fc2 = nn.Linear(64, num_classes)  # Output layer for style classification\n",
    "\n",
    "    def forward(self, style_embedding):\n",
    "        \"\"\"\n",
    "        Forward pass of the Style Classifier.\n",
    "\n",
    "        Args:\n",
    "            style_embedding: Tensor of shape [batch_size, style_dim].\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape [batch_size, num_classes] with logits for each class.\n",
    "        \"\"\"\n",
    "        x = F.relu(self.fc1(style_embedding))  # Hidden layer with ReLU activation\n",
    "        logits = self.fc2(x)  # Output layer (logits for style classes)\n",
    "        return logits  # Return logits directly\n",
    "\n",
    "class ContentClassifier(nn.Module):\n",
    "    def __init__(self, content_dim, vocab_size):\n",
    "        \"\"\"\n",
    "        Content Classifier to predict BoW features from the content embedding.\n",
    "\n",
    "        Args:\n",
    "            content_dim: Dimensionality of the content embedding (input size).\n",
    "            vocab_size: Size of the vocabulary (output size).\n",
    "        \"\"\"\n",
    "        super(ContentClassifier, self).__init__()\n",
    "        self.fc = nn.Linear(content_dim, vocab_size)  # Single-layer linear classifier\n",
    "\n",
    "    def forward(self, content_embedding):\n",
    "        \"\"\"\n",
    "        Forward pass of the Content Classifier.\n",
    "\n",
    "        Args:\n",
    "            content_embedding: Tensor of shape [batch_size, content_dim].\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape [batch_size, vocab_size] with probabilities for BoW features.\n",
    "        \"\"\"\n",
    "        return F.softmax(self.fc(content_embedding), dim=1)  # Softmax to get probabilities\n",
    "\n",
    "def J_mul_style(style_preds, style_labels):\n",
    "    \"\"\"\n",
    "    Multi-task style loss: Ensures style embedding captures style-specific information.\n",
    "\n",
    "    Args:\n",
    "        style_preds: Predicted logits for the positive class (Tensor of shape [batch_size, 1]).\n",
    "        style_labels: Ground truth probabilities for the positive class (Tensor of shape [batch_size, 1]).\n",
    "\n",
    "    Returns:\n",
    "        Multi-task style loss (scalar Tensor).\n",
    "    \"\"\"\n",
    "    # Apply binary cross-entropy with logits\n",
    "    loss = F.binary_cross_entropy_with_logits(style_preds.squeeze(), style_labels.float())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def J_adv_style(adv_style_preds, style_labels):\n",
    "    \"\"\"\n",
    "    Adversarial style loss: Ensures content embedding does not contain style-specific information.\n",
    "\n",
    "    Args:\n",
    "        adv_style_preds: Predicted logits for the positive class (Tensor of shape [batch_size, 1]).\n",
    "        style_labels: Ground truth probabilities for the positive class (Tensor of shape [batch_size, 1]).\n",
    "\n",
    "    Returns:\n",
    "        Adversarial style loss (scalar Tensor).\n",
    "    \"\"\"\n",
    "    # Negative Binary Cross-Entropy with logits to encourage fooling the classifier\n",
    "    loss = -F.binary_cross_entropy_with_logits(adv_style_preds.squeeze(), style_labels.float())\n",
    "    return loss\n",
    "\n",
    "def J_mul_content(content_preds, target_bow):\n",
    "    \"\"\"\n",
    "    Multi-task content loss: Ensures content embedding captures noun-based content information.\n",
    "\n",
    "    Args:\n",
    "        content_preds: Predicted probabilities for BoW nouns (Tensor of shape [batch_size, vocab_size]).\n",
    "        target_bow: Ground truth noun-based BoW vectors (Tensor of shape [batch_size, vocab_size]).\n",
    "\n",
    "    Returns:\n",
    "        Multi-task content loss (scalar Tensor).\n",
    "    \"\"\"\n",
    "    # Add a small epsilon to avoid log(0) errors\n",
    "    epsilon = 1e-8\n",
    "    loss = -torch.sum(target_bow * torch.log(content_preds + epsilon), dim=1).mean()\n",
    "    return loss\n",
    "\n",
    "def J_adv_content(adv_content_preds, target_bow):\n",
    "    \"\"\"\n",
    "    Adversarial content loss: Ensures style embedding does not contain noun-based content information.\n",
    "\n",
    "    Args:\n",
    "        adv_content_preds: Predicted probabilities for BoW nouns from the style embedding \n",
    "                           (Tensor of shape [batch_size, vocab_size]).\n",
    "        target_bow: Ground truth noun-based BoW vectors (Tensor of shape [batch_size, vocab_size]).\n",
    "\n",
    "    Returns:\n",
    "        Adversarial content loss (scalar Tensor).\n",
    "    \"\"\"\n",
    "    # Add a small epsilon to avoid log(0) errors\n",
    "    epsilon = 1e-8\n",
    "    loss = -torch.sum(target_bow * torch.log(adv_content_preds + epsilon), dim=1).mean()\n",
    "    return loss\n",
    "\n",
    "class DisentangledVAE(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, style_dim, content_dim, embedding_matrix=None):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        if embedding_matrix is not None:\n",
    "            self.embedding.weight.data.copy_(embedding_matrix)\n",
    "            self.embedding.weight.requires_grad = True\n",
    "        \n",
    "        self.padding_idx = 0\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder_rnn = nn.GRU(\n",
    "            embedding_dim, \n",
    "            hidden_dim,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            num_layers=2,\n",
    "            dropout=0.3\n",
    "        )\n",
    "        \n",
    "        # Multi-task components\n",
    "        self.style_encoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        self.content_encoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        # Latent projections\n",
    "        self.style_mu = nn.Linear(hidden_dim, style_dim)\n",
    "        self.style_logvar = nn.Linear(hidden_dim, style_dim)\n",
    "        self.content_mu = nn.Linear(hidden_dim, content_dim)\n",
    "        self.content_logvar = nn.Linear(hidden_dim, content_dim)\n",
    "        \n",
    "        # Decoder components\n",
    "        self.latent_to_hidden = nn.Linear(style_dim + content_dim, hidden_dim)\n",
    "        self.decoder_rnn = nn.GRU(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            batch_first=True,\n",
    "            num_layers=2,\n",
    "            dropout=0.3\n",
    "        )\n",
    "        self.output_fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def encode(self, x, lengths=None):\n",
    "        embedded = self.embedding(x)\n",
    "        output, hidden = self.encoder_rnn(embedded)\n",
    "        hidden = torch.cat([hidden[-2], hidden[-1]], dim=1)\n",
    "        \n",
    "        style_hidden = self.style_encoder(hidden)\n",
    "        content_hidden = self.content_encoder(hidden)\n",
    "        \n",
    "        style_mu = self.style_mu(style_hidden)\n",
    "        style_logvar = self.style_logvar(style_hidden)\n",
    "        content_mu = self.content_mu(content_hidden)\n",
    "        content_logvar = self.content_logvar(content_hidden)\n",
    "        \n",
    "        return style_mu, style_logvar, content_mu, content_logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5 * logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return mu + eps * std\n",
    "        return mu\n",
    "    \n",
    "    def decode(self, style, content, x):\n",
    "        batch_size = x.size(0)\n",
    "        max_len = x.size(1)\n",
    "        \n",
    "        # Initialize hidden state\n",
    "        hidden = self.latent_to_hidden(torch.cat([style, content], dim=1))\n",
    "        hidden = hidden.unsqueeze(0).repeat(2, 1, 1)\n",
    "        \n",
    "        # Teacher forcing\n",
    "        embedded = self.embedding(x)\n",
    "        output, _ = self.decoder_rnn(embedded, hidden)\n",
    "        output = self.output_fc(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def forward(self, x, lengths=None):\n",
    "        style_mu, style_logvar, content_mu, content_logvar = self.encode(x, lengths)\n",
    "        style = self.reparameterize(style_mu, style_logvar)\n",
    "        content = self.reparameterize(content_mu, content_logvar)\n",
    "        recon_x = self.decode(style, content, x)\n",
    "        return recon_x, style_mu, style_logvar, content_mu, content_logvar, style, content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "28b01b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model, optimizer, and scheduler are set up successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_185097/3510765379.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  vae.embedding.weight.data.copy_(torch.tensor(embedding_matrix))\n",
      "/home/qik/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Setup device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Step 2: Define the VAE model\n",
    "vocab_size = len(vocab)  # Replace with the actual vocab size\n",
    "embedding_dim = 300  # Embedding dimension\n",
    "\n",
    "style_dim = 128\n",
    "content_dim = 128\n",
    "hidden_dim = 256\n",
    "num_style_classes = 1\n",
    "vae = DisentangledVAE(vocab_size, embedding_dim=300, hidden_dim=256, style_dim=style_dim, content_dim=content_dim).to(device)  # Initialize the VAE model\n",
    "\n",
    "# Style Classifier\n",
    "style_classifier = StyleClassifier(style_dim=style_dim, num_classes=num_style_classes).to(device)\n",
    "\n",
    "# Content Classifier\n",
    "content_classifier = ContentClassifier(content_dim=content_dim, vocab_size=vocab_size).to(device)\n",
    "\n",
    "# Define separate optimizers for the classifiers\n",
    "style_optimizer = torch.optim.Adam(style_classifier.parameters(), lr=1e-3)\n",
    "content_optimizer = torch.optim.Adam(content_classifier.parameters(), lr=1e-3)\n",
    "\n",
    "# Step 3: Initialize the embedding layer with pre-trained embeddings if provided\n",
    "if embedding_matrix is not None:\n",
    "    # Copy the pre-trained embedding weights into the model\n",
    "    vae.embedding.weight.data.copy_(torch.tensor(embedding_matrix))\n",
    "else:\n",
    "    # If no embedding matrix is provided, initialize with random embeddings\n",
    "    vae.embedding.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "# Step 4: Define the optimizer\n",
    "learning_rate = 1e-3  # Set the learning rate\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=learning_rate)\n",
    "\n",
    "# Step 5: Define the learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min', \n",
    "    factor=0.5, \n",
    "    patience=2, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Print confirmation\n",
    "print(\"Model, optimizer, and scheduler are set up successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3a283e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def noun_bow(sentence, vocab):\n",
    "    \"\"\"\n",
    "    Create a Bag-of-Words (BoW) vector based on nouns in the sentence.\n",
    "\n",
    "    Args:\n",
    "        sentence: A string (input sentence).\n",
    "        vocab: Vocabulary (dictionary mapping nouns to indices).\n",
    "\n",
    "    Returns:\n",
    "        BoW vector (1D tensor of size vocab_size).\n",
    "    \"\"\"\n",
    "    # Tokenize and POS tag the sentence\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    \n",
    "    # Extract nouns based on POS tags\n",
    "    nouns = [word for word, tag in pos_tags if tag.startswith('NN')]  # NN, NNS, NNP, NNPS\n",
    "    \n",
    "    # Count noun occurrences\n",
    "    noun_counts = Counter(nouns)\n",
    "    \n",
    "    # Create the BoW vector\n",
    "    bow_vector = torch.zeros(len(vocab), dtype=torch.float32)\n",
    "    for noun, count in noun_counts.items():\n",
    "        if noun in vocab:\n",
    "            bow_vector[vocab[noun]] += count\n",
    "    return bow_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269e97fc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on cuda...\n",
      "Epoch 1/20\n",
      "  Batch 0/6926: Loss=10.7467, Recon=9.1808, KL=0.0000, Style=0.0692, Content=1.6551, Adv=-0.1585\n",
      "  Batch 100/6926: Loss=3.6707, Recon=2.4844, KL=0.0000, Style=0.0301, Content=1.4052, Adv=-0.2490\n",
      "  Batch 200/6926: Loss=2.2047, Recon=1.2099, KL=0.0000, Style=0.0106, Content=1.3177, Adv=-0.3334\n",
      "  Batch 300/6926: Loss=1.5922, Recon=0.7627, KL=0.0000, Style=0.0273, Content=1.1176, Adv=-0.3154\n",
      "  Batch 400/6926: Loss=1.3667, Recon=0.7158, KL=0.0000, Style=0.0124, Content=0.9454, Adv=-0.3068\n",
      "  Batch 500/6926: Loss=1.3535, Recon=0.5700, KL=0.0000, Style=0.0130, Content=1.0892, Adv=-0.3187\n",
      "  Batch 600/6926: Loss=1.0997, Recon=0.3796, KL=0.0000, Style=0.0161, Content=1.0892, Adv=-0.3851\n",
      "  Batch 700/6926: Loss=0.9356, Recon=0.3210, KL=0.0000, Style=0.0086, Content=0.8962, Adv=-0.2902\n",
      "  Batch 800/6926: Loss=1.0083, Recon=0.3419, KL=0.0000, Style=0.0078, Content=1.0257, Adv=-0.3671\n",
      "  Batch 900/6926: Loss=0.7342, Recon=0.2468, KL=0.0000, Style=0.0119, Content=0.7714, Adv=-0.2959\n",
      "  Batch 1000/6926: Loss=0.9919, Recon=0.2168, KL=0.0000, Style=0.0123, Content=1.1600, Adv=-0.3972\n",
      "  Batch 1100/6926: Loss=0.5569, Recon=0.1692, KL=0.0000, Style=0.0233, Content=0.6718, Adv=-0.3073\n",
      "  Batch 1200/6926: Loss=0.7117, Recon=0.1198, KL=0.0000, Style=0.0148, Content=0.9450, Adv=-0.3679\n",
      "  Batch 1300/6926: Loss=0.7239, Recon=0.1522, KL=0.0000, Style=0.0123, Content=0.9248, Adv=-0.3653\n",
      "  Batch 1400/6926: Loss=0.6923, Recon=0.1342, KL=0.0000, Style=0.0082, Content=0.8898, Adv=-0.3398\n",
      "  Batch 1500/6926: Loss=0.7098, Recon=0.1539, KL=0.0000, Style=0.0046, Content=0.9020, Adv=-0.3507\n",
      "  Batch 1600/6926: Loss=0.4408, Recon=0.0660, KL=0.0000, Style=0.0074, Content=0.6928, Adv=-0.3253\n",
      "  Batch 1700/6926: Loss=0.6667, Recon=0.1481, KL=0.0000, Style=0.0053, Content=0.8412, Adv=-0.3279\n",
      "  Batch 1800/6926: Loss=0.7418, Recon=0.0842, KL=0.0000, Style=0.0206, Content=1.0055, Adv=-0.3685\n",
      "  Batch 1900/6926: Loss=0.5507, Recon=0.0544, KL=0.0000, Style=0.0061, Content=0.8533, Adv=-0.3631\n",
      "  Batch 2000/6926: Loss=0.6680, Recon=0.0728, KL=0.0000, Style=0.0239, Content=0.9599, Adv=-0.3886\n",
      "  Batch 2100/6926: Loss=0.5244, Recon=0.0913, KL=0.0000, Style=0.0049, Content=0.7964, Adv=-0.3683\n",
      "  Batch 2200/6926: Loss=0.5612, Recon=0.1066, KL=0.0000, Style=0.0089, Content=0.7996, Adv=-0.3540\n",
      "  Batch 2300/6926: Loss=0.4930, Recon=0.0476, KL=0.0000, Style=0.0197, Content=0.7510, Adv=-0.3253\n",
      "  Batch 2400/6926: Loss=0.3306, Recon=0.0173, KL=0.0000, Style=0.0125, Content=0.6192, Adv=-0.3185\n",
      "  Batch 2500/6926: Loss=0.2616, Recon=0.0146, KL=0.0000, Style=0.0034, Content=0.5747, Adv=-0.3310\n",
      "  Batch 2600/6926: Loss=0.4679, Recon=0.0628, KL=0.0000, Style=0.0052, Content=0.7535, Adv=-0.3536\n",
      "  Batch 2700/6926: Loss=0.3979, Recon=0.0395, KL=0.0000, Style=0.0023, Content=0.6961, Adv=-0.3400\n",
      "  Batch 2800/6926: Loss=0.2990, Recon=0.0536, KL=0.0000, Style=0.0093, Content=0.5558, Adv=-0.3197\n",
      "  Batch 2900/6926: Loss=0.2805, Recon=0.0383, KL=0.0000, Style=0.0023, Content=0.5537, Adv=-0.3138\n",
      "  Batch 3000/6926: Loss=0.2533, Recon=0.0424, KL=0.0000, Style=0.0071, Content=0.5036, Adv=-0.2997\n",
      "  Batch 3100/6926: Loss=0.4390, Recon=0.0127, KL=0.0000, Style=0.0117, Content=0.8124, Adv=-0.3978\n",
      "  Batch 3200/6926: Loss=0.2807, Recon=0.0180, KL=0.0000, Style=0.0118, Content=0.6049, Adv=-0.3541\n",
      "  Batch 3300/6926: Loss=0.2715, Recon=0.0294, KL=0.0000, Style=0.0066, Content=0.5730, Adv=-0.3375\n",
      "  Batch 3400/6926: Loss=0.2853, Recon=0.0521, KL=0.0000, Style=0.0067, Content=0.5410, Adv=-0.3145\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starting training on {device}...\")\n",
    "# Training hyperparameters\n",
    "best_loss = float('inf')\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "\n",
    "# Begin training\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    \n",
    "    vae.train()\n",
    "    total_loss = 0\n",
    "    total_recon_loss = 0\n",
    "    total_kl_loss = 0\n",
    "    total_style_loss = 0\n",
    "    total_content_loss = 0\n",
    "    total_adv_loss = 0\n",
    "\n",
    "    # Iterate over the dataset\n",
    "    for batch_idx, (input_tokens, style_labels, lengths) in enumerate(data_loader):\n",
    "        input_tokens = input_tokens.to(device)\n",
    "        style_labels = style_labels.to(device)\n",
    "        id_to_token = {idx: token for token, idx in vocab.items()}\n",
    "        bow_labels = torch.stack([\n",
    "            noun_bow(\" \".join([id_to_token[idx.item()] for idx in input_seq if idx.item() in id_to_token]), vocab)\n",
    "            for input_seq in input_tokens\n",
    "        ]).to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        recon_x, style_mu, style_logvar, content_mu, content_logvar, style, content = vae(input_tokens, lengths)\n",
    "\n",
    "        # Reconstruction loss\n",
    "        recon_loss = F.cross_entropy(\n",
    "            recon_x.view(-1, recon_x.size(-1)), \n",
    "            input_tokens.view(-1), \n",
    "            ignore_index=0\n",
    "        )\n",
    "\n",
    "        # KL divergence with annealing\n",
    "        kl_weight = min(0.1, epoch / 20.0)  # Reduced maximum weight\n",
    "        kl_style = -0.5 * torch.sum(1 + style_logvar - style_mu.pow(2) - style_logvar.exp(), dim=1).mean()\n",
    "        kl_content = -0.5 * torch.sum(1 + content_logvar - content_mu.pow(2) - content_logvar.exp(), dim=1).mean()\n",
    "        kl_loss = (kl_style + kl_content) * kl_weight\n",
    "\n",
    "        # Multi-task style and content losses\n",
    "        style_preds = style_classifier(style)  # Style classifier predictions from style embedding\n",
    "        content_preds = content_classifier(content)  # Content classifier predictions from content embedding\n",
    "        #print(style_preds)\n",
    "        #print(style_labels)\n",
    "        style_loss = J_mul_style(style_preds, style_labels)\n",
    "        content_loss = J_mul_content(content_preds, bow_labels)\n",
    "\n",
    "        # Adversarial style and content losses\n",
    "        adv_style_preds = style_classifier(content)  # Style classifier applied to content embedding\n",
    "        adv_content_preds = content_classifier(style)  # Content classifier applied to style embedding\n",
    "        # the adversarial are mean to be optimized, so *(-1) for them\n",
    "        adv_style_loss = -J_adv_style(adv_style_preds, style_labels)\n",
    "        adv_content_loss = -J_adv_content(adv_content_preds, bow_labels)\n",
    "        adv_loss = (adv_style_loss + adv_content_loss) * 0.01  # Reduced adversarial weight\n",
    "\n",
    "        # Scale losses\n",
    "        style_loss *= 0.1\n",
    "        content_loss *= 0.1\n",
    "\n",
    "        # Total loss\n",
    "        loss = recon_loss + kl_loss + style_loss + content_loss + adv_loss\n",
    "\n",
    "        # Optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(vae.parameters(), max_norm=0.5)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update running totals\n",
    "        total_loss += loss.item()\n",
    "        total_recon_loss += recon_loss.item()\n",
    "        total_kl_loss += kl_loss.item()\n",
    "        total_style_loss += style_loss.item()\n",
    "        total_content_loss += content_loss.item()\n",
    "        total_adv_loss += adv_loss.item()\n",
    "\n",
    "        # Print batch metrics every 100 batches\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"  Batch {batch_idx}/{len(data_loader)}: \"\n",
    "                  f\"Loss={loss.item():.4f}, \"\n",
    "                  f\"Recon={recon_loss.item():.4f}, \"\n",
    "                  f\"KL={kl_loss.item():.4f}, \"\n",
    "                  f\"Style={style_loss.item():.4f}, \"\n",
    "                  f\"Content={content_loss.item():.4f}, \"\n",
    "                  f\"Adv={adv_loss.item():.4f}\")\n",
    "\n",
    "    # Compute average losses for the epoch\n",
    "    avg_total_loss = total_loss / len(data_loader)\n",
    "    avg_recon_loss = total_recon_loss / len(data_loader)\n",
    "    avg_kl_loss = total_kl_loss / len(data_loader)\n",
    "    avg_style_loss = total_style_loss / len(data_loader)\n",
    "    avg_content_loss = total_content_loss / len(data_loader)\n",
    "    avg_adv_loss = total_adv_loss / len(data_loader)\n",
    "\n",
    "    # Print detailed metrics for the epoch\n",
    "    print(f\"Epoch {epoch + 1} Summary:\")\n",
    "    print(f\"  Total Loss: {avg_total_loss:.4f}\")\n",
    "    print(f\"  Recon Loss: {avg_recon_loss:.4f}\")\n",
    "    print(f\"  KL Loss: {avg_kl_loss:.4f}\")\n",
    "    print(f\"  Style Loss: {avg_style_loss:.4f}\")\n",
    "    print(f\"  Content Loss: {avg_content_loss:.4f}\")\n",
    "    print(f\"  Adversarial Loss: {avg_adv_loss:.4f}\")\n",
    "\n",
    "    # Early stopping based on validation loss\n",
    "    if avg_total_loss < best_loss:\n",
    "        best_loss = avg_total_loss\n",
    "        patience_counter = 0\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': vae.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': best_loss,\n",
    "        }, 'best_model.pt')\n",
    "        print(f\"  Model saved with loss: {best_loss:.4f}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"  No improvement. Patience counter: {patience_counter}/{patience}\")\n",
    "\n",
    "    # Early stopping condition\n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "    # Step the learning rate scheduler\n",
    "    scheduler.step(avg_total_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6e8976c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/qik/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "nltk.download('punkt')\n",
    "\n",
    "def tokens_to_words(token_ids, vocab):\n",
    "    inv_vocab = {v: k for k, v in vocab.items()}\n",
    "    return [inv_vocab.get(token_id, '<UNK>') for token_id in token_ids if token_id not in [0, 2, 3]]\n",
    "\n",
    "def calculate_bleu_score(data_loader, model, vocab, device):\n",
    "    model.eval()\n",
    "    total_bleu_score = 0\n",
    "    num_sentences = 0\n",
    "    smoothing_fn = SmoothingFunction().method1\n",
    "    \n",
    "    print(\"\\nBLEU-S: Evaluating content preservation...\\n\")\n",
    "    with torch.no_grad():\n",
    "        for input_tokens, _, lengths in data_loader:\n",
    "            input_tokens = input_tokens.to(device)\n",
    "            recon_x, _, _, _, _, _, _ = model(input_tokens, lengths)\n",
    "            recon_x = recon_x.argmax(dim=-1)\n",
    "            \n",
    "            for i in range(min(5, len(input_tokens))):\n",
    "                original_sentence = tokens_to_words(input_tokens[i].tolist(), vocab)\n",
    "                reconstructed_sentence = tokens_to_words(recon_x[i].tolist(), vocab)\n",
    "                \n",
    "                if len(original_sentence) == 0 or len(reconstructed_sentence) == 0:\n",
    "                    continue\n",
    "                \n",
    "                print(f\"Original: {' '.join(original_sentence)}\")\n",
    "                print(f\"Reconstructed: {' '.join(reconstructed_sentence)}\\n\")\n",
    "                \n",
    "                weights = [0.25, 0.25, 0.25, 0.25]\n",
    "                bleu_score = sentence_bleu(\n",
    "                    [original_sentence], \n",
    "                    reconstructed_sentence, \n",
    "                    weights=weights,\n",
    "                    smoothing_function=smoothing_fn\n",
    "                )\n",
    "                total_bleu_score += bleu_score\n",
    "                num_sentences += 1\n",
    "            \n",
    "            if num_sentences >= 20:\n",
    "                break\n",
    "    \n",
    "    avg_bleu_score = total_bleu_score / num_sentences if num_sentences > 0 else 0\n",
    "    print(f\"Average BLEU-S Score: {avg_bleu_score:.4f}\")\n",
    "    return avg_bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c393f7c8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting Evaluation ===\n",
      "\n",
      "Training style classifier...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 87\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Starting Evaluation ===\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining style classifier...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 87\u001b[0m classifier \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_style_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Content Preservation (BLEU-S Score) ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     90\u001b[0m bleu_score \u001b[38;5;241m=\u001b[39m calculate_bleu_score(data_loader_test, vae, vocab, device)\n",
      "Cell \u001b[0;32mIn[60], line 2\u001b[0m, in \u001b[0;36mtrain_style_classifier\u001b[0;34m(data_loader, vocab_size, device)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_style_classifier\u001b[39m(data_loader, vocab_size, device):\n\u001b[0;32m----> 2\u001b[0m     classifier \u001b[38;5;241m=\u001b[39m \u001b[43mStyleClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      3\u001b[0m     criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCELoss()\n\u001b[1;32m      4\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(\n\u001b[1;32m      5\u001b[0m         classifier\u001b[38;5;241m.\u001b[39mparameters(),\n\u001b[1;32m      6\u001b[0m         lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0003\u001b[39m,\n\u001b[1;32m      7\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m\n\u001b[1;32m      8\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "def train_style_classifier(data_loader, vocab_size, device):\n",
    "    classifier = StyleClassifier(vocab_size, 512, 256).to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        classifier.parameters(),\n",
    "        lr=0.0003,\n",
    "        weight_decay=0.1\n",
    "    )\n",
    "    \n",
    "    classifier.train()\n",
    "    print(\"\\nTraining Style Classifier...\\n\")\n",
    "    \n",
    "    for epoch in range(10):\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for input_tokens, labels, _ in data_loader:\n",
    "            input_tokens = input_tokens.to(device)\n",
    "            labels = labels.to(device).float()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            predictions = classifier(input_tokens).squeeze()\n",
    "            \n",
    "            smoothed_labels = labels * 0.9 + 0.05\n",
    "            loss = criterion(predictions, smoothed_labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(classifier.parameters(), max_norm=0.5)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            preds = (predictions > 0.5).float()\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "        \n",
    "        avg_loss = total_loss / len(data_loader)\n",
    "        accuracy = correct / total\n",
    "        print(f\"Epoch {epoch + 1}/10, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    return classifier\n",
    "\n",
    "def evaluate_style_transfer(data_loader, model, classifier, vocab, device):\n",
    "    model.eval()\n",
    "    classifier.eval()\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    print(\"\\nEvaluating Style Transfer Accuracy...\\n\")\n",
    "    with torch.no_grad():\n",
    "        for input_tokens, labels, lengths in data_loader:\n",
    "            input_tokens = input_tokens.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Get reconstructed text\n",
    "            recon_x, _, _, _, _, style, _ = model(input_tokens, lengths)\n",
    "            recon_x = recon_x.argmax(dim=-1)\n",
    "            \n",
    "            # Use the separate classifier for style predictions\n",
    "            style_predictions = classifier(recon_x).squeeze()\n",
    "            style_labels = (style_predictions > 0.5).float()\n",
    "            \n",
    "            correct_predictions += (style_labels == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "            \n",
    "            # Print examples\n",
    "            for i in range(min(5, len(input_tokens))):\n",
    "                original_sentence = tokens_to_words(input_tokens[i].tolist(), vocab)\n",
    "                reconstructed_sentence = tokens_to_words(recon_x[i].tolist(), vocab)\n",
    "                \n",
    "                if len(original_sentence) > 0 and len(reconstructed_sentence) > 0:\n",
    "                    print(f\"Original: {' '.join(original_sentence)}\")\n",
    "                    print(f\"Reconstructed: {' '.join(reconstructed_sentence)}\")\n",
    "                    print(f\"Style Prediction: {style_predictions[i].item():.4f}, True Style: {labels[i].item()}\\n\")\n",
    "            \n",
    "            if total_predictions >= 100:\n",
    "                break\n",
    "    \n",
    "    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "    print(f\"Style Transfer Accuracy: {accuracy:.4f}\")\n",
    "    return accuracy\n",
    "\n",
    "# Run evaluation\n",
    "print(\"\\n=== Starting Evaluation ===\\n\")\n",
    "    \n",
    "print(\"Training style classifier...\")\n",
    "classifier = train_style_classifier(data_loader, len(vocab), device)\n",
    "\n",
    "print(\"\\n=== Content Preservation (BLEU-S Score) ===\")\n",
    "bleu_score = calculate_bleu_score(data_loader_test, vae, vocab, device)\n",
    "\n",
    "print(\"\\n=== Style Transfer Accuracy ===\")\n",
    "style_transfer_accuracy = evaluate_style_transfer(data_loader_test, vae, classifier, vocab, device)\n",
    "\n",
    "print(\"\\n=== Final Results ===\")\n",
    "print(f\"BLEU-S Score: {bleu_score:.4f}\")\n",
    "print(f\"Style Transfer Accuracy: {style_transfer_accuracy:.4f}\")\n",
    "\n",
    "def run_evaluation(data_loader_train, data_loader_test, model, vocab, vocab_size, device):\n",
    "    print(\"\\n=== Starting Evaluation ===\\n\")\n",
    "    \n",
    "    print(\"Training style classifier...\")\n",
    "    classifier = train_style_classifier(data_loader_train, vocab_size, device)\n",
    "    \n",
    "    print(\"\\n=== Content Preservation (BLEU-S Score) ===\")\n",
    "    bleu_score = calculate_bleu_score(data_loader_test, model, vocab, device)\n",
    "    \n",
    "    print(\"\\n=== Style Transfer Accuracy ===\")\n",
    "    style_transfer_accuracy = evaluate_style_transfer(data_loader_test, model, classifier, vocab, device)\n",
    "    \n",
    "    print(\"\\n=== Final Results ===\")\n",
    "    print(f\"BLEU-S Score: {bleu_score:.4f}\")\n",
    "    print(f\"Style Transfer Accuracy: {style_transfer_accuracy:.4f}\")\n",
    "    \n",
    "    return bleu_score, style_transfer_accuracy\n",
    "\n",
    "# Run the evaluation\n",
    "print(\"Starting evaluation pipeline...\")\n",
    "bleu_score, style_accuracy = run_evaluation(\n",
    "    data_loader, \n",
    "    data_loader_test, \n",
    "    vae, \n",
    "    vocab, \n",
    "    len(vocab), \n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f9c308",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "2f1a0099",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b5009e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b726625",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348b1211",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9707ce74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
