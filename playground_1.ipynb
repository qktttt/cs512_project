{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm  \n",
    "from dataloader import *\n",
    "from model import * \n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import nltk \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "target_confidence = 0.8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    inputs, labels = zip(*batch)\n",
    "    max_length = max(len(seq) for seq in inputs)\n",
    "    \n",
    "    # Convert each sequence to a list, pad with 0, and convert to tensor\n",
    "    padded_inputs = [torch.cat([seq, torch.zeros(max_length - len(seq), dtype=torch.long)]) for seq in inputs]\n",
    "    lengths = [len(seq) for seq in inputs]\n",
    "    \n",
    "    return torch.stack(padded_inputs), torch.tensor(labels, dtype=torch.float), lengths\n",
    "\n",
    "def tokens_to_words(token_ids, vocab):\n",
    "    inv_vocab = {v: k for k, v in vocab.items()}\n",
    "    return [inv_vocab.get(token_id, '<UNK>') for token_id in token_ids if token_id != 0]  # Exclude padding\n",
    "\n",
    "\n",
    "class TextDatasetTest(Dataset):\n",
    "    def __init__(self, data_dir, vocab):\n",
    "        super(TextDatasetTest, self).__init__()\n",
    "        self.data = []\n",
    "        self.vocab = vocab\n",
    "\n",
    "        # Load data from the files\n",
    "        files = [\"sentiment.test.0\", \"sentiment.test.1\"]\n",
    "        for filename in files:\n",
    "            file_path = os.path.join(data_dir, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                lines = f.readlines()\n",
    "                for line in lines:\n",
    "                    tokens = line.strip().split()\n",
    "                    label = 1 if filename.endswith('.1') else 0  # Binary label\n",
    "                    self.data.append((tokens, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens, label = self.data[idx]\n",
    "        token_ids = [self.vocab.get(token, self.vocab['<UNK>']) for token in tokens]\n",
    "        return torch.tensor(token_ids, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
    "    \n",
    "data_dir = \"./data/sentiment_style_transfer/yelp\"\n",
    "vocab = build_vocab(data_dir)\n",
    "dataset = TextDatasetTest(data_dir, vocab)\n",
    "data_loader = DataLoader(dataset, batch_size=64, collate_fn=collate_fn, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisentangledVAE(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, style_dim, content_dim):\n",
    "        super(DisentangledVAE, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder_rnn = nn.GRU(embedding_dim, hidden_dim, \n",
    "                                 batch_first=True, \n",
    "                                 bidirectional=True,\n",
    "                                 num_layers=2,\n",
    "                                 dropout=0.2)\n",
    "        \n",
    "        # Latent spaces\n",
    "        self.style_encoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        self.content_encoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        # Mean and logvar projections\n",
    "        self.style_mu = nn.Linear(hidden_dim, style_dim)\n",
    "        self.style_logvar = nn.Linear(hidden_dim, style_dim)\n",
    "        self.content_mu = nn.Linear(hidden_dim, content_dim)\n",
    "        self.content_logvar = nn.Linear(hidden_dim, content_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.latent_to_hidden = nn.Sequential(\n",
    "            nn.Linear(style_dim + content_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        self.decoder_rnn = nn.GRU(embedding_dim + hidden_dim, hidden_dim,\n",
    "                                 batch_first=True,\n",
    "                                 num_layers=2,\n",
    "                                 dropout=0.2)\n",
    "        \n",
    "        self.output_fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "        # Style classifier for adversarial training\n",
    "        self.style_classifier = nn.Sequential(\n",
    "            nn.Linear(style_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def encode(self, x, lengths=None):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Embed input\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        # Pack for variable length sequences\n",
    "        if lengths is not None:\n",
    "            embedded = nn.utils.rnn.pack_padded_sequence(\n",
    "                embedded, lengths, batch_first=True, enforce_sorted=False\n",
    "            )\n",
    "        \n",
    "        # Encode\n",
    "        _, hidden = self.encoder_rnn(embedded)\n",
    "        # Combine bidirectional states\n",
    "        hidden = torch.cat([hidden[-2], hidden[-1]], dim=1)\n",
    "        \n",
    "        # Encode style and content\n",
    "        style_hidden = self.style_encoder(hidden)\n",
    "        content_hidden = self.content_encoder(hidden)\n",
    "        \n",
    "        # Get latent parameters\n",
    "        style_mu = self.style_mu(style_hidden)\n",
    "        style_logvar = self.style_logvar(style_hidden)\n",
    "        content_mu = self.content_mu(content_hidden)\n",
    "        content_logvar = self.content_logvar(content_hidden)\n",
    "        \n",
    "        return style_mu, style_logvar, content_mu, content_logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "        \n",
    "    def decode(self, style, content, x):\n",
    "        batch_size = x.size(0)\n",
    "        max_len = x.size(1)\n",
    "        \n",
    "        # Combine latent vectors\n",
    "        latent = torch.cat([style, content], dim=1)\n",
    "        hidden = self.latent_to_hidden(latent)\n",
    "        \n",
    "        # Initialize decoder hidden state\n",
    "        hidden = hidden.unsqueeze(0).repeat(2, 1, 1)  # num_layers * batch * hidden\n",
    "        \n",
    "        # Teacher forcing with concatenated latent\n",
    "        embedded = self.embedding(x)\n",
    "        hidden_expanded = hidden[-1].unsqueeze(1).repeat(1, max_len, 1)\n",
    "        decoder_input = torch.cat([embedded, hidden_expanded], dim=2)\n",
    "        \n",
    "        # Decode\n",
    "        outputs, _ = self.decoder_rnn(decoder_input, hidden)\n",
    "        outputs = self.output_fc(outputs)\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "    def forward(self, x, lengths=None):\n",
    "        # Encode\n",
    "        style_mu, style_logvar, content_mu, content_logvar = self.encode(x, lengths)\n",
    "        \n",
    "        # Sample latent vectors\n",
    "        style = self.reparameterize(style_mu, style_logvar)\n",
    "        content = self.reparameterize(content_mu, content_logvar)\n",
    "        \n",
    "        # Decode\n",
    "        recon_x = self.decode(style, content, x)\n",
    "        \n",
    "        return recon_x, style_mu, style_logvar, content_mu, content_logvar, style, content\n",
    "\n",
    "    def classify_style(self, style):\n",
    "        return self.style_classifier(style)\n",
    "\n",
    "def vae_loss(recon_x, x, style_mu, style_logvar, content_mu, content_logvar):\n",
    "    recon_loss = F.cross_entropy(recon_x.view(-1, recon_x.size(-1)), x.view(-1), ignore_index=0)  # Reconstruction loss\n",
    "    kl_style = -0.5 * torch.sum(1 + style_logvar - style_mu.pow(2) - style_logvar.exp())  # KL divergence for style\n",
    "    kl_content = -0.5 * torch.sum(1 + content_logvar - content_mu.pow(2) - content_logvar.exp())  # KL divergence for content\n",
    "    return recon_loss + kl_style + kl_content\n",
    "\n",
    "def multi_task_loss(style_preds, style_labels, content_preds, content_labels):\n",
    "    style_loss = F.cross_entropy(style_preds, style_labels)  # Style classification loss\n",
    "    content_loss = F.cross_entropy(content_preds, content_labels)  # Content classification loss\n",
    "    return style_loss + content_loss\n",
    "\n",
    "def adversarial_loss(style_preds, content_preds):\n",
    "    adversarial_style_loss = -F.cross_entropy(style_preds, torch.zeros_like(style_preds))  # Fool style classifier\n",
    "    adversarial_content_loss = -F.cross_entropy(content_preds, torch.zeros_like(content_preds))  # Fool content classifier\n",
    "    return adversarial_style_loss + adversarial_content_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 8.8364\n",
      "Epoch 2/10, Loss: 6.3012\n",
      "Epoch 3/10, Loss: 5.4021\n",
      "Epoch 4/10, Loss: 4.5849\n",
      "Epoch 5/10, Loss: 3.7619\n",
      "Epoch 6/10, Loss: 3.0867\n",
      "Epoch 7/10, Loss: 2.5593\n",
      "Epoch 8/10, Loss: 2.1242\n",
      "Epoch 9/10, Loss: 1.7939\n",
      "Epoch 10/10, Loss: 1.5404\n"
     ]
    }
   ],
   "source": [
    "# Modified training loop\n",
    "def train_vae_with_dataset(vae, optimizer, data_loader, device):\n",
    "    \"\"\"\n",
    "    Train the VAE with a custom dataset.\n",
    "    \"\"\"\n",
    "    vae.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for input_tokens, style_labels, lengths in data_loader:\n",
    "        # Move data to the appropriate device\n",
    "        input_tokens = input_tokens.to(device)\n",
    "        style_labels = style_labels.to(device)\n",
    "        \n",
    "        # Forward pass through the model\n",
    "        recon_x, style_mu, style_logvar, content_mu, content_logvar, style, content = vae(input_tokens, lengths)\n",
    "        \n",
    "        # Calculate reconstruction loss\n",
    "        loss_vae = F.cross_entropy(recon_x.view(-1, recon_x.size(-1)), input_tokens.view(-1), ignore_index=0)\n",
    "        \n",
    "        # KL divergence losses\n",
    "        kl_style = -0.5 * torch.sum(1 + style_logvar - style_mu.pow(2) - style_logvar.exp())\n",
    "        kl_content = -0.5 * torch.sum(1 + content_logvar - content_mu.pow(2) - content_logvar.exp())\n",
    "        \n",
    "        # Style classification loss\n",
    "        style_preds = vae.classify_style(style).squeeze()  # Make sure predictions are the right shape\n",
    "        style_labels = style_labels.float()  # Convert to float\n",
    "        loss_multi_task = F.binary_cross_entropy(style_preds, style_labels)\n",
    "        \n",
    "        # Total loss\n",
    "        loss = loss_vae + 0.1 * (kl_style + kl_content) + loss_multi_task\n",
    "        \n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(vae.parameters(), max_norm=1.0)  # Add gradient clipping\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "# Training parameters\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 256\n",
    "hidden_dim = 512\n",
    "style_dim = 32\n",
    "content_dim = 256\n",
    "learning_rate = 5e-4\n",
    "epochs = 10\n",
    "\n",
    "# Initialize model and optimizer\n",
    "vae = DisentangledVAE(vocab_size, embedding_dim, hidden_dim, style_dim, content_dim).to(device)\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    loss = train_vae_with_dataset(vae, optimizer, data_loader, device)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleClassifier(nn.Module):\n",
    "    def _init_(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(StyleClassifier, self)._init_()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.encoder = nn.GRU(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, hidden = self.encoder(x)\n",
    "        # Combine bidirectional states\n",
    "        hidden = torch.cat([hidden[-2], hidden[-1]], dim=1)\n",
    "        return self.fc(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UPDATED PART ABOVE, NOT UPDATED PART BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model_complete.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_113390/3363134432.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_1 = torch.load('model_complete.pth')\n"
     ]
    }
   ],
   "source": [
    "model_1 = torch.load('model_complete.pth') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: \t\t\t they only received one star because you have to provide a rating .\n",
      "Reconstructed Sentence: \t\t they only received one star because you have to provide a rating .\n",
      "\n",
      "Original Sentence: \t\t\t always takes way too long even if you 're the only one there .\n",
      "Reconstructed Sentence: \t\t always takes way too long even if you 're the only one there .\n",
      "\n",
      "Original Sentence: \t\t\t she could not and would not explain herself .\n",
      "Reconstructed Sentence: \t\t she could not and would not explain herself .\n",
      "\n",
      "Original Sentence: \t\t\t all she did was give me the run around and lied and bs everything .\n",
      "Reconstructed Sentence: \t\t all she did was give me the run around and lied and bs everything .\n",
      "\n",
      "Original Sentence: \t\t\t it does not take that long to cook sliders !\n",
      "Reconstructed Sentence: \t\t it does not take that long to cook sliders !\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Inspect some sentences from the data loader\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "model_1.eval()\n",
    "with torch.no_grad():\n",
    "    for input_tokens, _, lengths in data_loader_test:\n",
    "        input_tokens = input_tokens.to(device)\n",
    "        x_reconstructed, _, _, _ = model_1(input_tokens)\n",
    "        x_reconstructed = x_reconstructed.argmax(dim=-1)  # Get the predicted token IDs\n",
    "\n",
    "        # Print a few input and output sentences\n",
    "        for i in range(5):  # Print 5 examples\n",
    "            original_sentence = tokens_to_words(input_tokens[i].tolist(), vocab)\n",
    "            reconstructed_sentence = tokens_to_words(x_reconstructed[i].tolist(), vocab)\n",
    "\n",
    "            print(\"Original Sentence: \\t\\t\\t\", \" \".join(original_sentence))\n",
    "            print(\"Reconstructed Sentence: \\t\\t\", \" \".join(reconstructed_sentence))\n",
    "            print()\n",
    "\n",
    "        break  # Only inspect the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_style_classifier(data_loader, vocab_size, device):\n",
    "    classifier = StyleClassifier(vocab_size, 300, 128).to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)\n",
    "\n",
    "    classifier.train()\n",
    "    for epoch in range(20):  # Train for a few epochs\n",
    "        total_loss = 0\n",
    "        for input_tokens, labels, _ in data_loader:  # Adjusted to unpack three values\n",
    "            input_tokens = input_tokens.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            predictions = classifier(input_tokens)\n",
    "            loss = criterion(predictions, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1}/5, Loss: {total_loss / len(data_loader)}\")\n",
    "    \n",
    "    return classifier\n",
    "\n",
    "def evaluate_style_transfer(data_loader, model, classifier, device):\n",
    "    model.eval()\n",
    "    classifier.eval()\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_tokens, labels, _ in data_loader:\n",
    "            input_tokens = input_tokens.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Get the reconstructed sentences\n",
    "            x_reconstructed, _, _, _ = model(input_tokens)\n",
    "            x_reconstructed = x_reconstructed.argmax(dim=-1)\n",
    "\n",
    "            # Predict the style of the reconstructed sentences\n",
    "            style_predictions = classifier(x_reconstructed)\n",
    "            style_labels = (style_predictions > 0.5).float()\n",
    "            \n",
    "            correct_predictions += (style_labels == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "    \n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    print(f\"Style Transfer Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m classifier \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_style_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m evaluate_style_transfer(data_loader_test, model_1, classifier, device)\n",
      "Cell \u001b[0;32mIn[16], line 9\u001b[0m, in \u001b[0;36mtrain_style_classifier\u001b[0;34m(data_loader, vocab_size, device)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m20\u001b[39m):  \u001b[38;5;66;03m# Train for a few epochs\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m input_tokens, labels, _ \u001b[38;5;129;01min\u001b[39;00m data_loader:  \u001b[38;5;66;03m# Adjusted to unpack three values\u001b[39;00m\n\u001b[1;32m     10\u001b[0m         input_tokens \u001b[38;5;241m=\u001b[39m input_tokens\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m         labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 18\u001b[0m, in \u001b[0;36mcollate_fn\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     15\u001b[0m padded_inputs \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mcat([seq, torch\u001b[38;5;241m.\u001b[39mzeros(max_length \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(seq), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)]) \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m inputs]\n\u001b[1;32m     16\u001b[0m lengths \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlen\u001b[39m(seq) \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m inputs]\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(padded_inputs), \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m)\u001b[49m, lengths\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "classifier = train_style_classifier(data_loader, len(vocab), device)\n",
    "evaluate_style_transfer(data_loader_test, model_1, classifier, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/qik/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Style Classifier...\n",
      "\n",
      "Epoch 1/5, Loss: 0.0954\n",
      "Epoch 2/5, Loss: 0.0522\n",
      "Epoch 3/5, Loss: 0.0387\n",
      "Epoch 4/5, Loss: 0.0300\n",
      "Epoch 5/5, Loss: 0.0241\n",
      "Epoch 6/5, Loss: 0.0205\n",
      "Epoch 7/5, Loss: 0.0178\n",
      "Epoch 8/5, Loss: 0.0158\n",
      "Epoch 9/5, Loss: 0.0145\n",
      "Epoch 10/5, Loss: 0.0139\n",
      "Epoch 11/5, Loss: 0.0132\n",
      "Epoch 12/5, Loss: 0.0124\n",
      "Epoch 13/5, Loss: 0.0116\n",
      "Epoch 14/5, Loss: 0.0117\n",
      "Epoch 15/5, Loss: 0.0116\n",
      "Epoch 16/5, Loss: 0.0113\n",
      "Epoch 17/5, Loss: 0.0113\n",
      "Epoch 18/5, Loss: 0.0108\n",
      "Epoch 19/5, Loss: 0.0105\n",
      "Epoch 20/5, Loss: 0.0104\n",
      "\n",
      "--- BLEU-S Score (Content Preservation) ---\n",
      "\n",
      "BLEU-S: Evaluating content preservation...\n",
      "\n",
      "Original: at this location the service was terrible .\n",
      "Reconstructed: so brand scale crumbs crappy steady shogun shogun adds adds adds adds adds adds adds\n",
      "\n",
      "Original: i ordered garlic bread and fettuccine alfredo pasta with vegetables .\n",
      "Reconstructed: anyways a+ cancer crusted cancer middle towing crusted middle watered belgian camarones inspector inspector inspector\n",
      "\n",
      "Original: i did n't even eat it .\n",
      "Reconstructed: warranties diamonds fusion covers masseuse increased winner volunteers volunteers volunteers inspector inspector inspector inspector inspector\n",
      "\n",
      "Original: this is a golf course that is tucked away it is in great condition .\n",
      "Reconstructed: renee renee renee nadine weekend dead renee renee tim liquid kitschy renee renee sites cause\n",
      "\n",
      "Original: the new management team is horrible !\n",
      "Reconstructed: a1 flagged active robbed marks marks kidding kidding noodle versus versus versus versus versus versus\n",
      "\n",
      "Average BLEU-S Score: 0.0000\n",
      "\n",
      "--- Style Transfer Accuracy ---\n",
      "\n",
      "Evaluating Style Transfer Accuracy...\n",
      "\n",
      "Original: the chips and guacamole were excellent too !\n",
      "Reconstructed: starts starts mm mm randomly sees struck mm niece victor victor victor victor victor saturday\n",
      "Style Prediction: 1.0, True Style: 1.0\n",
      "\n",
      "Original: but it may not actually be in stock anyway .\n",
      "Reconstructed: just ohhh domino incompetence attempted lion ohhh litchfield men picked picked elevators elevators elevators elevators\n",
      "Style Prediction: 0.0, True Style: 0.0\n",
      "\n",
      "Original: they also have lost sight of what good deli food is .\n",
      "Reconstructed: pairs people stars jewlery facilities forthright foundation fred rex facilities styrofoam plain selling proof proof\n",
      "Style Prediction: 0.0, True Style: 0.0\n",
      "\n",
      "Original: if i was the manager , i 'd fire that kid on the spot .\n",
      "Reconstructed: charts carr delighted brick beverage biscotti smell lucked cassie veterinarians restaraunt restaraunt glendale reliable styling\n",
      "Style Prediction: 1.0, True Style: 0.0\n",
      "\n",
      "Original: the food here is bland and boring and bad .\n",
      "Reconstructed: tummy represent assist dressy dressy vegetarian tl cucumber vegetarian vegetarian bananas cause cause cause cause\n",
      "Style Prediction: 1.0, True Style: 0.0\n",
      "\n",
      "Style Transfer Accuracy: 0.5469\n",
      "\n",
      "--- Final Results ---\n",
      "BLEU-S Score: 0.0000\n",
      "Style Transfer Accuracy: 0.5469\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 0.546875)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### updated code \n",
    "import torch\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def tokens_to_words(token_ids, vocab):\n",
    "    inv_vocab = {v: k for k, v in vocab.items()}\n",
    "    return [inv_vocab.get(token_id, '<UNK>') for token_id in token_ids if token_id != 0]\n",
    "\n",
    "def calculate_bleu_score(data_loader, model, vocab, device):\n",
    "    model.eval()\n",
    "    total_bleu_score = 0\n",
    "    num_sentences = 0\n",
    "    smoothing_fn = SmoothingFunction().method1\n",
    "\n",
    "    print(\"\\nBLEU-S: Evaluating content preservation...\\n\")\n",
    "    with torch.no_grad():\n",
    "        for input_tokens, _, lengths in data_loader:\n",
    "            input_tokens = input_tokens.to(device)\n",
    "            x_reconstructed, _, _, _ = model(input_tokens)\n",
    "            x_reconstructed = x_reconstructed.argmax(dim=-1)\n",
    "\n",
    "            for i in range(min(5, len(input_tokens))):  \n",
    "                original_sentence = tokens_to_words(input_tokens[i].tolist(), vocab)\n",
    "                reconstructed_sentence = tokens_to_words(x_reconstructed[i].tolist(), vocab)\n",
    "                print(f\"Original: {' '.join(original_sentence)}\")\n",
    "                print(f\"Reconstructed: {' '.join(reconstructed_sentence)}\\n\")\n",
    "\n",
    "                bleu_score = sentence_bleu([original_sentence], reconstructed_sentence, smoothing_function=smoothing_fn)\n",
    "                total_bleu_score += bleu_score\n",
    "                num_sentences += 1\n",
    "\n",
    "            break  # Evaluate only on the first batch for now\n",
    "\n",
    "    avg_bleu_score = total_bleu_score / num_sentences if num_sentences > 0 else 0\n",
    "    print(f\"Average BLEU-S Score: {avg_bleu_score:.4f}\")\n",
    "    return avg_bleu_score\n",
    "\n",
    "def train_style_classifier(data_loader, vocab_size, device):\n",
    "    classifier = StyleClassifier(vocab_size, 300, 128).to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)\n",
    "\n",
    "    classifier.train()\n",
    "    print(\"\\nTraining Style Classifier...\\n\")\n",
    "    for epoch in range(20):\n",
    "        total_loss = 0\n",
    "        for input_tokens, labels, _ in data_loader:\n",
    "            input_tokens = input_tokens.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            predictions = classifier(input_tokens)\n",
    "            loss = criterion(predictions, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1}/5, Loss: {total_loss / len(data_loader):.4f}\")\n",
    "    \n",
    "    return classifier\n",
    "\n",
    "def evaluate_style_transfer(data_loader, model, classifier, vocab, device):\n",
    "    model.eval()\n",
    "    classifier.eval()\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    print(\"\\nEvaluating Style Transfer Accuracy...\\n\")\n",
    "    with torch.no_grad():\n",
    "        for input_tokens, labels, _ in data_loader:\n",
    "            input_tokens = input_tokens.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            x_reconstructed, _, _, _ = model(input_tokens)\n",
    "            x_reconstructed = x_reconstructed.argmax(dim=-1)\n",
    "\n",
    "            style_predictions = classifier(x_reconstructed)\n",
    "            style_labels = (style_predictions > 0.5).float()\n",
    "            correct_predictions += (style_labels == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "\n",
    "            for i in range(min(5, len(input_tokens))):\n",
    "                original_sentence = tokens_to_words(input_tokens[i].tolist(), vocab)\n",
    "                reconstructed_sentence = tokens_to_words(x_reconstructed[i].tolist(), vocab)\n",
    "                print(f\"Original: {' '.join(original_sentence)}\")\n",
    "                print(f\"Reconstructed: {' '.join(reconstructed_sentence)}\")\n",
    "                print(f\"Style Prediction: {style_labels[i].item()}, True Style: {labels[i].item()}\\n\")\n",
    "\n",
    "            break  # Evaluate only on the first batch for now\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "    print(f\"Style Transfer Accuracy: {accuracy:.4f}\")\n",
    "    return accuracy\n",
    "\n",
    "def run_evaluation(data_loader_train, data_loader_test, model, vocab, vocab_size, device):\n",
    "    # Train Style Classifier\n",
    "    classifier = train_style_classifier(data_loader_train, vocab_size, device)\n",
    "\n",
    "    print(\"\\n--- BLEU-S Score (Content Preservation) ---\")\n",
    "    bleu_score = calculate_bleu_score(data_loader_test, model, vocab, device)\n",
    "\n",
    "    print(\"\\n--- Style Transfer Accuracy ---\")\n",
    "    style_transfer_accuracy = evaluate_style_transfer(data_loader_test, model, classifier, vocab, device)\n",
    "\n",
    "    print(\"\\n--- Final Results ---\")\n",
    "    print(f\"BLEU-S Score: {bleu_score:.4f}\")\n",
    "    print(f\"Style Transfer Accuracy: {style_transfer_accuracy:.4f}\")\n",
    "\n",
    "    return bleu_score, style_transfer_accuracy\n",
    "\n",
    "run_evaluation(data_loader, data_loader_test, model, vocab, len(vocab), device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
